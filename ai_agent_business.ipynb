{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8446bbea"
      },
      "source": [
        "## Set up the development environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "MPZH6NXNc9sp"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai langchain beautifulsoup4 requests google-search-results kaggle huggingface-hub PyGithub\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from serpapi import GoogleSearch\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "from huggingface_hub import HfApi\n",
        "from github import Github, RateLimitExceededException\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "4vq-VdPUnMP3",
        "outputId": "605a4166-7563-4734-9ee8-4598c9891c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your kaggle.json file.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4301f18c-02cb-41c5-961a-99f886860ffb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4301f18c-02cb-41c5-961a-99f886860ffb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API key is configured.\n"
          ]
        }
      ],
      "source": [
        "# Upload kaggle.json file\n",
        "from google.colab import files\n",
        "print(\"Please upload your kaggle.json file.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Set up the Kaggle API\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Kaggle API key is configured.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMzaoe-znTXV",
        "outputId": "01ba5f1a-a46d-4155-e68a-8ded41d6f0c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API keys are set.\n"
          ]
        }
      ],
      "source": [
        "# Set API keys from Colab Secrets\n",
        "try:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    raise ValueError(\"OpenAI API Key not found in Colab Secrets. Please add it.\")\n",
        "\n",
        "try:\n",
        "    os.environ[\"SERPAPI_API_KEY\"] = userdata.get(\"SERPAPI_API_KEY\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    raise ValueError(\"SerpApi API Key not found in Colab Secrets. Please add it.\")\n",
        "\n",
        "try:\n",
        "    os.environ[\"GITHUB_TOKEN\"] = userdata.get(\"GITHUB_TOKEN\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"Warning: GitHub token not found in Colab Secrets. GitHub search will be skipped.\")\n",
        "\n",
        "try:\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"Warning: Hugging Face token not found in Colab Secrets. Hugging Face search will be skipped.\")\n",
        "\n",
        "print(\"API keys are set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "207eac89"
      },
      "source": [
        "## Develop the research agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "837c8402",
        "outputId": "c93d4c77-c5ef-45c6-8fee-be9cb83bec74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running research agent for: google\n",
            "Scraped: https://about.google/\n",
            "Failed https://finance.yahoo.com/quote/GOOG/profile/ — Status 404\n",
            "Scraped: https://www.forbes.com/companies/google/\n",
            "\n",
            " Sample scraped documents:\n",
            "Title: About Google: Our products, technology and company ...\n",
            "URL: https://about.google/\n",
            "Text snippet: Jump to Content Explore our products and features across Search, Google Workspace and more Learn all about our leading AI models â and discover their capabilities See how weâre tackling some of th...\n",
            "\n",
            "Title: Google | GOOG Stock Price, Company Overview & News\n",
            "URL: https://www.forbes.com/companies/google/\n",
            "Text snippet: Upgrade to gain access to exclusive features and add your profile photo. Learn More Forbes does not accept payment for placement on lists....\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def research_agent_run(company_or_industry, max_results=3):\n",
        "    \"\"\"\n",
        "    Performs a real-time web search for the given company or industry using SerpApi,\n",
        "    and scrapes content from the top search results with improved anti-bot handling.\n",
        "\n",
        "    Args:\n",
        "        company_or_industry (str): The name of the company or industry to research.\n",
        "        max_results (int): Number of top search results to scrape.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: Each dict contains 'url', 'title', and 'text' fields.\n",
        "    \"\"\"\n",
        "    print(f\"Running research agent for: {company_or_industry}\")\n",
        "    search_query = f\"{company_or_industry} company profile and recent news\"\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": search_query,\n",
        "        \"api_key\": os.getenv(\"SERPAPI_API_KEY\")\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        search = GoogleSearch(params)\n",
        "        results = search.get_dict()\n",
        "\n",
        "        # Check for SerpApi error\n",
        "        if \"error\" in results:\n",
        "            print(f\"SerpApi Error: {results['error']}\")\n",
        "            return []\n",
        "\n",
        "        organic_results = results.get(\"organic_results\", [])\n",
        "        if not organic_results:\n",
        "            print(\"No organic results found.\")\n",
        "            return []\n",
        "\n",
        "        scraped_docs = []\n",
        "\n",
        "        # Commonly accepted headers to mimic a browser\n",
        "        headers_template = {\n",
        "            \"User-Agent\": random.choice([\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0 Safari/537.36\",\n",
        "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n",
        "                \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0 Safari/537.36\"\n",
        "            ]),\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Referer\": \"https://www.google.com/\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "            \"Connection\": \"keep-alive\"\n",
        "        }\n",
        "\n",
        "        session = requests.Session()\n",
        "\n",
        "        for i, result in enumerate(organic_results[:max_results]):\n",
        "            url = result.get(\"link\")\n",
        "            title = result.get(\"title\")\n",
        "            if not url or not title:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Add a small random sleep to avoid bot detection\n",
        "                time.sleep(random.uniform(2, 5))\n",
        "\n",
        "                response = session.get(url, headers=headers_template, timeout=15)\n",
        "                if response.status_code == 200:\n",
        "                    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "                    paragraphs = soup.find_all(\"p\")\n",
        "                    text = \" \".join(p.get_text() for p in paragraphs)\n",
        "                    clean_text = \" \".join(text.split()).strip()\n",
        "\n",
        "                    if clean_text:\n",
        "                        scraped_docs.append({\n",
        "                            \"url\": url,\n",
        "                            \"title\": title,\n",
        "                            \"text\": clean_text\n",
        "                        })\n",
        "                        print(f\"Scraped: {url}\")\n",
        "                elif response.status_code == 403:\n",
        "                    print(f\"403 Forbidden at {url}. Skipping...\")\n",
        "                else:\n",
        "                    print(f\"Failed {url} — Status {response.status_code}\")\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error scraping {url}: {e}\")\n",
        "\n",
        "        return scraped_docs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error in research agent: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- Quick test ---\n",
        "if __name__ == \"__main__\":\n",
        "    test_company = \"google\"\n",
        "    research_docs = research_agent_run(test_company, max_results=3)\n",
        "    if research_docs:\n",
        "        print(\"\\n Sample scraped documents:\")\n",
        "        for doc in research_docs:\n",
        "            print(f\"Title: {doc['title']}\")\n",
        "            print(f\"URL: {doc['url']}\")\n",
        "            print(f\"Text snippet: {doc['text'][:200]}...\\n\")\n",
        "    else:\n",
        "        print(\"No documents found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a55d72"
      },
      "source": [
        "## Develop the market standards & use case generation agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e6e312e",
        "outputId": "fe4c1194-f6d2-479e-e4a2-3abfb1f4c072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Use Cases:\n",
            "{'title': 'Intelligent Customer Support Chatbots', 'description': 'Deploy AI-driven chatbots to provide real-time support to users across Google Workspace and Search, enhancing user satisfaction and reducing response times.', 'data sources': 'Customer interaction logs, support ticket data, user feedback, conversation transcripts', 'impact': 'High', 'complexity': 'Medium'}\n",
            "{'title': 'Predictive Analytics for Ad Spend Optimization', 'description': 'Utilize AI to analyze historical ad performance data and predict the most effective allocation of advertising budgets across platforms to maximize ROI.', 'data sources': 'Ad performance data, user engagement metrics, market trends, competitor analysis', 'impact': 'High', 'complexity': 'High'}\n",
            "{'title': 'Personalized Content Recommendations', 'description': 'Implement generative AI algorithms to curate personalized content suggestions for users based on their search history and interactions across Google services.', 'data sources': 'User search history, browsing behavior, content engagement metrics, demographic data', 'impact': 'Medium', 'complexity': 'Medium'}\n",
            "{'title': 'Automated Document Summarization in Google Workspace', 'description': 'Develop AI tools that automatically summarize lengthy documents and emails within Google Workspace, improving productivity for users.', 'data sources': 'Document data, email content, user interaction data, previous summaries', 'impact': 'Medium', 'complexity': 'Medium'}\n",
            "{'title': 'Sentiment Analysis for Product Feedback', 'description': 'Leverage AI to analyze user feedback and sentiment across Google products, allowing for timely adjustments and enhancements based on user insights.', 'data sources': 'Customer feedback data, product reviews, social media mentions, support ticket data', 'impact': 'Medium', 'complexity': 'High'}\n"
          ]
        }
      ],
      "source": [
        "def usecase_agent_generate_usecases(company_name, research_findings):\n",
        "    \"\"\"\n",
        "    Analyzes research findings and proposes relevant AI/GenAI use cases\n",
        "    using a language model.\n",
        "\n",
        "    Args:\n",
        "        company_name (str): The name of the company.\n",
        "        research_findings (list): A list of dictionaries containing research data.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, each representing a proposed use case.\n",
        "    \"\"\"\n",
        "    company_context = \"\\n\".join([d.get('text', '') for d in research_findings if d.get('text')])\n",
        "\n",
        "    if not company_context:\n",
        "        print(\"Warning: No research context available to generate use cases.\")\n",
        "        return []\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an AI strategy consultant. Given these facts about {company_name} (context below), propose exactly 5 distinct GenAI/AI use cases for the company focusing on operations, customer experience, and monetization.\n",
        "\n",
        "For each use case, provide the following information in a structured format:\n",
        "TITLE: [Short Title]\n",
        "DESCRIPTION: [One-sentence description]\n",
        "DATA SOURCES: [List of required data sources, e.g., customer data, sales data, website logs]\n",
        "BUSINESS IMPACT: [Expected business impact: Low, Medium, or High]\n",
        "COMPLEXITY: [Estimated complexity: Low, Medium, or High]\n",
        "\n",
        "Context:\n",
        "{company_context}\n",
        "\n",
        "Ensure each use case is clearly separated by a horizontal rule \"---\" and follows the exact 'FIELD_NAME: [Value]' format. Do not include any introductory or concluding text outside of the use case blocks.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        client = openai.OpenAI()\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        llm_output = resp.choices[0].message.content\n",
        "\n",
        "        use_cases = []\n",
        "        use_case_blocks = llm_output.strip().split('---')\n",
        "\n",
        "        for block in use_case_blocks:\n",
        "            block = block.strip()\n",
        "            if not block:\n",
        "                continue\n",
        "\n",
        "            current_use_case = {}\n",
        "            lines = block.split('\\n')\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if line.startswith(\"TITLE:\"):\n",
        "                    current_use_case[\"title\"] = line.replace(\"TITLE:\", \"\").strip()\n",
        "                elif line.startswith(\"DESCRIPTION:\"):\n",
        "                    current_use_case[\"description\"] = line.replace(\"DESCRIPTION:\", \"\").strip()\n",
        "                elif line.startswith(\"DATA SOURCES:\"):\n",
        "                    current_use_case[\"data sources\"] = line.replace(\"DATA SOURCES:\", \"\").strip()\n",
        "                elif line.startswith(\"BUSINESS IMPACT:\"):\n",
        "                    current_use_case[\"impact\"] = line.replace(\"BUSINESS IMPACT:\", \"\").strip()\n",
        "                elif line.startswith(\"COMPLEXITY:\"):\n",
        "                    current_use_case[\"complexity\"] = line.replace(\"COMPLEXITY:\", \"\").strip()\n",
        "\n",
        "            if current_use_case and current_use_case.get(\"title\"):\n",
        "                use_cases.append(current_use_case)\n",
        "\n",
        "        if not use_cases or len(use_cases) < 3:\n",
        "             print(\"Warning: Parsing failed or fewer than 3 valid use cases found.\")\n",
        "             print(\"LLM Output:\\n\", llm_output)\n",
        "\n",
        "        return use_cases\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating use cases: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test with the documents from the previous cell\n",
        "if 'research_docs' in locals() and research_docs:\n",
        "    generated_use_cases = usecase_agent_generate_usecases(test_company, research_docs)\n",
        "    print(\"\\nGenerated Use Cases:\")\n",
        "    for uc in generated_use_cases:\n",
        "        print(uc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a8546c5"
      },
      "source": [
        "## Develop the resource asset collection agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "VUFTHjWOmMC9"
      },
      "outputs": [],
      "source": [
        "# --- Agent Functionality ---\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "def search_kaggle(keyword, max_results=2):\n",
        "    \"\"\"\n",
        "    Searches for datasets on Kaggle using the Kaggle API,\n",
        "    retrieving basic info like URL, title, downloads, and dataset size.\n",
        "    \"\"\"\n",
        "    print(f\"Searching Kaggle for datasets related to: {keyword}\")\n",
        "    results = []\n",
        "    try:\n",
        "        api = KaggleApi()\n",
        "        api.authenticate()\n",
        "\n",
        "        search_results = api.dataset_list(search=keyword, sort_by=\"hottest\")\n",
        "\n",
        "        if not search_results:\n",
        "            print(f\"No datasets found on Kaggle for '{keyword}'\")\n",
        "            return []\n",
        "\n",
        "        for ds in search_results[:max_results]:\n",
        "            dataset_info = {\n",
        "                \"url\": f\"https://www.kaggle.com/{ds.ref}\",\n",
        "                \"title\": getattr(ds, \"title\", \"Untitled Dataset\"),\n",
        "                \"notes\": f\"Kaggle Dataset - Downloads: {getattr(ds, 'downloadCount', 'N/A')}, Views: {getattr(ds, 'viewCount', 'N/A')}\"\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                # Fetch file info for dataset size\n",
        "                files_in_dataset = api.dataset_list_files(ds.ref)\n",
        "                if files_in_dataset and hasattr(files_in_dataset, \"datasetFiles\"):\n",
        "                    total_size_bytes = sum(\n",
        "                        getattr(f, \"totalBytes\", 0) for f in files_in_dataset.datasetFiles\n",
        "                    )\n",
        "                    total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "                    dataset_info[\"notes\"] += f\", Total Size: {total_size_mb:.2f} MB\"\n",
        "                else:\n",
        "                    dataset_info[\"notes\"] += \", Size info not available\"\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  - Error getting file list for {ds.ref}: {e}\")\n",
        "                dataset_info[\"notes\"] += \", Error getting size info\"\n",
        "\n",
        "            results.append(dataset_info)\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching Kaggle: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_huggingface(keyword, max_results=2):\n",
        "    \"\"\"\n",
        "    Searches for real datasets on Hugging Face using the Hugging Face Hub API.\n",
        "    \"\"\"\n",
        "    print(f\"Searching Hugging Face for datasets related to: {keyword}\")\n",
        "    results = []\n",
        "    try:\n",
        "        api = HfApi()\n",
        "        datasets = api.list_datasets(search=keyword, sort=\"downloads\", limit=max_results)\n",
        "\n",
        "        for ds in datasets:\n",
        "            results.append({\n",
        "                \"url\": f\"https://huggingface.co/datasets/{ds.id}\",\n",
        "                \"title\": ds.id.split('/')[-1],\n",
        "                \"notes\": f\"HuggingFace Dataset, downloads: {ds.downloads}\"\n",
        "            })\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching Hugging Face: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_github(keyword, max_results=2):\n",
        "    \"\"\"\n",
        "    Searches for real GitHub repositories using the GitHub API.\n",
        "    \"\"\"\n",
        "    print(f\"Searching GitHub for repositories related to: {keyword}\")\n",
        "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
        "    if not github_token:\n",
        "        print(\"GitHub token not found. Skipping GitHub search.\")\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        from github import Github, RateLimitExceededException, Auth\n",
        "        auth = Auth.Token(github_token)\n",
        "        g = Github(auth=auth)\n",
        "\n",
        "        query = f\"{keyword} dataset\" # Add \" dataset\" to the search query\n",
        "        # Search for repositories, limiting results per page\n",
        "        repositories = g.search_repositories(query=query, sort=\"stars\", order=\"desc\", per_page=max_results)\n",
        "\n",
        "\n",
        "        for i, repo in enumerate(repositories):\n",
        "            # The per_page parameter should handle limiting, but a safeguard loop is fine.\n",
        "            if i >= max_results:\n",
        "                break\n",
        "            results.append({\n",
        "                \"url\": repo.html_url,\n",
        "                \"title\": repo.full_name,\n",
        "                \"notes\": f\"GitHub Repo, stars: {repo.stargazers_count}\"\n",
        "            })\n",
        "        return results\n",
        "    except RateLimitExceededException:\n",
        "        print(\"GitHub API rate limit exceeded. Please wait or use a token with a higher limit.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching GitHub: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def dataset_agent_find_datasets(use_cases):\n",
        "    \"\"\"\n",
        "    Searches for relevant datasets and resources for proposed use cases\n",
        "    from Kaggle, Hugging Face, and GitHub.\n",
        "    \"\"\"\n",
        "    updated_use_cases = []\n",
        "    for uc in use_cases:\n",
        "        keywords = []\n",
        "        # Extract keywords from title and description, filter out short/common words\n",
        "        all_words = []\n",
        "        if uc.get(\"title\"):\n",
        "            all_words.extend(uc[\"title\"].lower().replace(':', '').split())\n",
        "        if uc.get(\"description\"):\n",
        "            all_words.extend(uc[\"description\"].lower().replace(':', '').split())\n",
        "\n",
        "        # Simple filtering of short words and basic stopwords\n",
        "        stop_words = ['for', 'and', 'with', 'the', 'from', 'about', 'this', 'that', 'which', 'using', 'based', 'improve', 'enhance', 'generate', 'automate', 'predict', 'implement', 'utilize', 'leverage']\n",
        "        keywords = [word for word in all_words if len(word) > 2 and word not in stop_words]\n",
        "\n",
        "\n",
        "        search_keywords = \" \".join(list(set(keywords))[:5]) # Take up to 5 unique keywords\n",
        "\n",
        "        datasets = []\n",
        "        # Call the corrected search functions\n",
        "        datasets.extend(search_kaggle(search_keywords))\n",
        "        datasets.extend(search_huggingface(search_keywords))\n",
        "        datasets.extend(search_github(search_keywords))\n",
        "\n",
        "        uc['datasets'] = datasets\n",
        "        updated_use_cases.append(uc)\n",
        "    return updated_use_cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a4e4ac8"
      },
      "source": [
        "## Develop the prioritization and final proposal component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6a3c92",
        "outputId": "9bcdd4cd-7e2d-430a-9f85-4c8e2ba5cb35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prioritized Use Cases (with scores):\n",
            "Score: 1.20 - Smart Ad Optimization\n",
            "Score: 0.90 - Predictive Customer Support\n",
            "Score: 0.90 - Personalized Content Recommendations\n",
            "Score: 0.70 - AI-Driven Market Analysis\n",
            "Score: 0.40 - Enhanced Workspace Collaboration Tools\n",
            "Markdown report saved to prioritized_usecases.md\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def prioritizer_rank(use_cases):\n",
        "    \"\"\"\n",
        "    Ranks use cases based on impact and complexity.\n",
        "    Uses a simplified scoring based on the provided formula structure.\n",
        "    \"\"\"\n",
        "    ranked_use_cases = []\n",
        "    for uc in use_cases:\n",
        "        # Assign numerical scores\n",
        "        impact_score = 0\n",
        "        if 'impact' in uc:\n",
        "            impact_lower = uc['impact'].lower()\n",
        "            if 'high' in impact_lower:\n",
        "                impact_score = 3\n",
        "            elif 'med' in impact_lower:\n",
        "                impact_score = 2\n",
        "            elif 'low' in impact_lower:\n",
        "                impact_score = 1\n",
        "\n",
        "        complexity_score = 0\n",
        "        if 'complexity' in uc:\n",
        "            complexity_lower = uc['complexity'].lower()\n",
        "            if 'high' in complexity_lower:\n",
        "                complexity_score = 1 # Inverse score for complexity\n",
        "            elif 'med' in complexity_lower:\n",
        "                complexity_score = 2\n",
        "            elif 'low' in complexity_lower:\n",
        "                complexity_score = 3\n",
        "\n",
        "        # Assuming data availability is medium (score 2) if not explicitly provided\n",
        "        # A more sophisticated approach would analyze data sources for availability\n",
        "        data_avail_score = 2\n",
        "\n",
        "        # Calculate core score using the provided formula structure\n",
        "        # core = 0.5 * impact_score + 0.4 * data_avail_score - 0.3 * complexity_score\n",
        "        # Simplified formula using only impact and complexity as data_avail_score is assumed constant\n",
        "        core_score = 0.5 * impact_score - 0.3 * complexity_score\n",
        "\n",
        "        uc['core_score'] = core_score\n",
        "        ranked_use_cases.append(uc)\n",
        "\n",
        "    # Sort by core score in descending order\n",
        "    ranked_use_cases = sorted(ranked_use_cases, key=lambda x: x.get('core_score', 0), reverse=True)\n",
        "\n",
        "    return ranked_use_cases\n",
        "\n",
        "def writer_save_markdown(use_cases, filename):\n",
        "    \"\"\"\n",
        "    Formats and saves the prioritized use cases as a markdown report.\n",
        "    \"\"\"\n",
        "    markdown_output = \"# Prioritized AI/GenAI Use Case Proposal\\n\\n\"\n",
        "\n",
        "    for i, uc in enumerate(use_cases):\n",
        "        markdown_output += f\"## {i+1}. {uc.get('title', 'Untitled Use Case')}\\n\\n\"\n",
        "        markdown_output += f\"**Description:** {uc.get('description', 'N/A')}\\n\\n\"\n",
        "        markdown_output += f\"**Required Data Sources:** {uc.get('data sources', 'N/A')}\\n\\n\"\n",
        "        markdown_output += f\"**Expected Business Impact:** {uc.get('impact', 'N/A')} | **Estimated Complexity:** {uc.get('complexity', 'N/A')}\\n\\n\"\n",
        "\n",
        "        if uc.get('datasets'):\n",
        "            markdown_output += \"**Relevant Resources:**\\n\"\n",
        "            for dataset in uc['datasets']:\n",
        "                title = dataset.get('title', 'Link')\n",
        "                url = dataset.get('url', '#') # Use '#' as a fallback URL\n",
        "                notes = dataset.get('notes', '')\n",
        "                markdown_output += f\"- [{title}]({url}) ({notes})\\n\"\n",
        "            markdown_output += \"\\n\" # Add an extra newline after resources\n",
        "\n",
        "        markdown_output += \"---\\n\\n\" # Separator between use cases\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(markdown_output)\n",
        "        print(f\"Markdown report saved to {filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error saving markdown report to {filename}: {e}\")\n",
        "\n",
        "# Test the functions with the use_cases_with_datasets variable\n",
        "if 'use_cases_with_datasets' in locals() and use_cases_with_datasets:\n",
        "    prioritized_usecases = prioritizer_rank(use_cases_with_datasets)\n",
        "    print(\"\\nPrioritized Use Cases (with scores):\")\n",
        "    for uc in prioritized_usecases:\n",
        "        print(f\"Score: {uc.get('core_score', 'N/A'):.2f} - {uc.get('title', 'Untitled')}\")\n",
        "\n",
        "    writer_save_markdown(prioritized_usecases, 'prioritized_usecases.md')\n",
        "else:\n",
        "    print(\"Skipping prioritization and markdown writing: use_cases_with_datasets not found or empty.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9b4ef81"
      },
      "source": [
        "## Integrate the agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "5e46ff87"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def orchestrator(company_or_industry):\n",
        "    \"\"\"\n",
        "    Orchestrates the multi-agent system workflow.\n",
        "\n",
        "    Args:\n",
        "        company_or_industry: The name of the company or industry to research.\n",
        "\n",
        "    Returns:\n",
        "        A list of prioritized use cases with associated data and resources.\n",
        "    \"\"\"\n",
        "    print(f\"Starting orchestration for: {company_or_industry}\")\n",
        "\n",
        "    # 1. Research Phase\n",
        "    print(\"Running research agent...\")\n",
        "    research_docs = research_agent_run(company_or_industry)\n",
        "    if not research_docs:\n",
        "        print(\"Research phase failed or returned no documents.\")\n",
        "        return []\n",
        "\n",
        "    # 2. Use Case Generation Phase\n",
        "    print(\"Running use case generation agent...\")\n",
        "    generated_use_cases = usecase_agent_generate_usecases(company_or_industry, research_docs)\n",
        "    if not generated_use_cases:\n",
        "        print(\"Use case generation phase failed or returned no use cases.\")\n",
        "        return []\n",
        "\n",
        "    # 3. Resource Collection Phase\n",
        "    print(\"Running dataset agent...\")\n",
        "    use_cases_with_datasets = dataset_agent_find_datasets(generated_use_cases)\n",
        "    if not use_cases_with_datasets:\n",
        "         print(\"Dataset agent failed or returned no datasets.\")\n",
        "         # Continue with use cases without datasets if the agent fails\n",
        "         use_cases_with_datasets = generated_use_cases\n",
        "\n",
        "\n",
        "    # 4. Prioritization Phase\n",
        "    print(\"Running prioritization agent...\")\n",
        "    prioritized_usecases = prioritizer_rank(use_cases_with_datasets)\n",
        "    if not prioritized_usecases:\n",
        "         print(\"Prioritization agent failed.\")\n",
        "         # Continue with the list from the previous step if prioritization fails\n",
        "         prioritized_usecases = use_cases_with_datasets\n",
        "\n",
        "\n",
        "    # 5. Report Writing Phase\n",
        "    print(\"Saving markdown report...\")\n",
        "    # Ensure the outputs directory exists\n",
        "    output_dir = \"outputs\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    output_filename = os.path.join(output_dir, f\"{company_or_industry.replace(' ', '_').lower()}_usecases.md\")\n",
        "    writer_save_markdown(prioritized_usecases, output_filename)\n",
        "\n",
        "    print(f\"Orchestration complete. Report saved to {output_filename}\")\n",
        "\n",
        "    return prioritized_usecases\n",
        "\n",
        "# Example of how to run the orchestrator\n",
        "# orchestrated_results = orchestrator(\"Tech Innovations Inc.\")\n",
        "# print(\"\\nFinal Prioritized Use Cases:\")\n",
        "# for uc in orchestrated_results:\n",
        "#     print(f\"{uc.get('core_score', 'N/A'):.2f} - {uc.get('title', 'Untitled')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af16db08",
        "outputId": "1cebc227-598d-458e-e123-bfa13f58f20a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting orchestration for: Global Solutions Corp\n",
            "Running research agent...\n",
            "Running research agent for: Global Solutions Corp\n",
            "Scraped: https://www.bloomberg.com/profile/company/0710124D:US\n",
            "403 Forbidden at https://pitchbook.com/profiles/company/60178-42. Skipping...\n",
            "403 Forbidden at https://www.zoominfo.com/c/global-solutions-inc/345259630. Skipping...\n",
            "Running use case generation agent...\n",
            "Running dataset agent...\n",
            "Searching Kaggle for datasets related to: data financial market insights analyze\n",
            "Searching Hugging Face for datasets related to: data financial market insights analyze\n",
            "Searching GitHub for repositories related to: data financial market insights analyze\n",
            "Searching Kaggle for datasets related to: preferences. clients personalized financial profiles\n",
            "Searching Hugging Face for datasets related to: preferences. clients personalized financial profiles\n",
            "Searching GitHub for repositories related to: preferences. clients personalized financial profiles\n",
            "Searching Kaggle for datasets related to: data report financial effort reducing\n",
            "Searching Hugging Face for datasets related to: data report financial effort reducing\n",
            "Searching GitHub for repositories related to: data report financial effort reducing\n",
            "Searching Kaggle for datasets related to: machine clients' conditions. market predictive\n",
            "No datasets found on Kaggle for 'machine clients' conditions. market predictive'\n",
            "Searching Hugging Face for datasets related to: machine clients' conditions. market predictive\n",
            "Searching GitHub for repositories related to: machine clients' conditions. market predictive\n",
            "Searching Kaggle for datasets related to: conditions behavior. financial models real-time\n",
            "Searching Hugging Face for datasets related to: conditions behavior. financial models real-time\n",
            "Searching GitHub for repositories related to: conditions behavior. financial models real-time\n",
            "Running prioritization agent...\n",
            "Saving markdown report...\n",
            "Markdown report saved to outputs/global_solutions_corp_usecases.md\n",
            "Orchestration complete. Report saved to outputs/global_solutions_corp_usecases.md\n",
            "\n",
            "Final Prioritized Use Cases:\n",
            "Score: 1.20 - Predictive Analytics for Client Needs\n",
            "Score: 0.90 - Intelligent Market Insights\n",
            "Score: 0.90 - Automated Report Generation\n",
            "Score: 0.70 - Dynamic Pricing Models\n",
            "Score: 0.40 - Personalized Customer Engagement\n"
          ]
        }
      ],
      "source": [
        "# Test the orchestrator function\n",
        "orchestrated_results = orchestrator(\"Global Solutions Corp\")\n",
        "print(\"\\nFinal Prioritized Use Cases:\")\n",
        "for uc in orchestrated_results:\n",
        "    print(f\"Score: {uc.get('core_score', 'N/A'):.2f} - {uc.get('title', 'Untitled')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2edd6303"
      },
      "source": [
        "## Build a user interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c761ad3",
        "outputId": "baf4ad43-79f0-43ff-caf5-5e30217daae8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-12 13:40:56.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.811 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.814 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.816 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.817 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.818 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.821 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.823 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.825 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.828 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.829 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.830 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.831 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-12 13:40:56.832 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Ensure the orchestrator function is defined (it was defined in the previous step)\n",
        "# if 'orchestrator' not in locals():\n",
        "#     st.error(\"Orchestrator function not found. Please ensure the previous steps were run.\")\n",
        "# else:\n",
        "st.title(\"Market Research & GenAI Use Case Agent\")\n",
        "\n",
        "company = st.text_input(\"Enter company name or industry:\")\n",
        "\n",
        "if st.button(\"Generate Use Cases\"):\n",
        "    if not company:\n",
        "        st.warning(\"Please enter a company name or industry.\")\n",
        "    # Check if orchestrator function is available before calling\n",
        "    elif 'orchestrator' in locals() and callable(orchestrator):\n",
        "        st.info(f\"Running analysis for {company}...\")\n",
        "        # Add a spinner or progress indicator for better UX\n",
        "        with st.spinner('Generating use cases...'):\n",
        "            prioritized_usecases = orchestrator(company)\n",
        "\n",
        "        if prioritized_usecases:\n",
        "            st.success(\"Analysis complete! Prioritized Use Cases:\")\n",
        "            for i, uc in enumerate(prioritized_usecases):\n",
        "                st.subheader(f\"{i+1}. {uc.get('title', 'Untitled Use Case')}\")\n",
        "                st.write(f\"**Core Score:** {uc.get('core_score', 'N/A'):.2f}\")\n",
        "                st.write(f\"**Description:** {uc.get('description', 'N/A')}\")\n",
        "                st.write(f\"**Required Data Sources:** {uc.get('data sources', 'N/A')}\")\n",
        "                st.write(f\"**Expected Business Impact:** {uc.get('impact', 'N/A')} | **Estimated Complexity:** {uc.get('complexity', 'N/A')}\")\n",
        "\n",
        "                if uc.get('datasets'):\n",
        "                    st.write(\"**Relevant Resources:**\")\n",
        "                    for dataset in uc['datasets']:\n",
        "                        title = dataset.get('title', 'Link')\n",
        "                        url = dataset.get('url', '#')\n",
        "                        notes = dataset.get('notes', '')\n",
        "                        # Display as a clickable link\n",
        "                        st.markdown(f\"- [{title}]({url}) ({notes})\")\n",
        "                st.markdown(\"---\") # Separator\n",
        "        else:\n",
        "            st.warning(\"Could not generate use cases for the provided company/industry.\")\n",
        "    else:\n",
        "        st.error(\"Orchestrator function not found or not callable.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46627ed5",
        "outputId": "da18d6f9-b701-4238-ed5a-e69ae353d4d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streamlit app code written to app.py\n"
          ]
        }
      ],
      "source": [
        "# Write the Streamlit app code to a file\n",
        "streamlit_code = \"\"\"\n",
        "import streamlit as st\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure the orchestrator function and its dependencies are available\n",
        "# Add the notebook directory to the path (for Colab/Notebook -> Streamlit integration)\n",
        "notebook_dir = '/content'\n",
        "if notebook_dir not in sys.path:\n",
        "    sys.path.append(notebook_dir)\n",
        "\n",
        "try:\n",
        "    from __main__ import (\n",
        "        orchestrator,\n",
        "        research_agent_run,\n",
        "        usecase_agent_generate_usecases,\n",
        "        dataset_agent_find_datasets,\n",
        "        prioritizer_rank,\n",
        "        writer_save_markdown,\n",
        "        search_kaggle,\n",
        "        search_huggingface,\n",
        "        search_github\n",
        "    )\n",
        "    print(\"Successfully imported notebook functions.\")\n",
        "except ImportError as e:\n",
        "    st.error(f\"Could not import notebook functions. Ensure notebook cells are run. Error: {e}\")\n",
        "    orchestrator = None\n",
        "\n",
        "st.title(\"Market Research & GenAI Use Case Agent\")\n",
        "\n",
        "company = st.text_input(\"Enter company name or industry:\")\n",
        "\n",
        "if st.button(\"Generate Use Cases\"):\n",
        "    if not company:\n",
        "        st.warning(\" Please enter a company name or industry.\")\n",
        "    elif orchestrator is not None and callable(orchestrator):\n",
        "        st.info(f\"Running analysis for **{company}** ...\")\n",
        "        with st.spinner('Generating use cases...'):\n",
        "            prioritized_usecases = orchestrator(company)\n",
        "\n",
        "        if prioritized_usecases:\n",
        "            st.success(\"Analysis complete! Prioritized Use Cases:\")\n",
        "            for i, uc in enumerate(prioritized_usecases):\n",
        "                st.subheader(f\"{i+1}. {uc.get('title', 'Untitled Use Case')}\")\n",
        "                st.write(f\"**Core Score:** {uc.get('core_score', 'N/A')}\")\n",
        "                st.write(f\"**Description:** {uc.get('description', 'N/A')}\")\n",
        "                st.write(f\"**Required Data Sources:** {uc.get('data sources', 'N/A')}\")\n",
        "                st.write(f\"**Expected Business Impact:** {uc.get('impact', 'N/A')} | **Estimated Complexity:** {uc.get('complexity', 'N/A')}\")\n",
        "\n",
        "                if uc.get('datasets'):\n",
        "                    st.write(\"**Relevant Resources:**\")\n",
        "                    for dataset in uc['datasets']:\n",
        "                        title = dataset.get('title', 'Link')\n",
        "                        url = dataset.get('url', '#')\n",
        "                        notes = dataset.get('notes', '')\n",
        "                        st.markdown(f\"- [{title}]({url}) ({notes})\")\n",
        "\n",
        "                st.markdown(\"---\")  # Separator\n",
        "        else:\n",
        "            st.warning(\"Could not generate use cases. Check logs for errors.\")\n",
        "    else:\n",
        "        st.error(\"Orchestrator function is not available. Please run the notebook cells first.\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"Streamlit app code written to app.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3aa0277",
        "outputId": "bef77321-a190-4c62-cac4-04bbdc6ea18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://199f84295a9e.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Install streamlit_colab for easy launching in Colab\n",
        "!pip install -q streamlit_colab\n",
        "\n",
        "# Launch the Streamlit app\n",
        "from streamlit_colab import run_streamlit\n",
        "from google.colab import userdata # Import userdata to access secrets\n",
        "\n",
        "# Get the ngrok token from Colab secrets\n",
        "try:\n",
        "    ngrok_token = userdata.get(\"NGROK_TOKEN\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"NGROK_TOKEN not found in Colab Secrets. Please add it to run Streamlit.\")\n",
        "    ngrok_token = None # Set token to None if not found\n",
        "\n",
        "# Run Streamlit only if the ngrok token is available\n",
        "if ngrok_token:\n",
        "    run_streamlit(\"app.py\", ngrok_token=ngrok_token)\n",
        "else:\n",
        "    print(\"Streamlit app cannot be launched without the NGROK_TOKEN.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73acfad2"
      },
      "source": [
        "## Test and refine the system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61fdbdf5",
        "outputId": "d1c8ceec-9498-4c31-ce2a-fd5578bdeaed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running research agent for: tesla\n",
            "403 Forbidden at https://www.tesla.com/blog. Skipping...\n",
            "Scraped: https://finance.yahoo.com/quote/TSLA/profile/\n",
            "Scraped: https://www.forbes.com/companies/tesla/\n",
            "\n",
            "--- Research Documents ---\n",
            "[{'url': 'https://finance.yahoo.com/quote/TSLA/profile/', 'title': 'Tesla, Inc. (TSLA) Company Profile & Facts - Yahoo Finance', 'text': 'Oops, something went wrong Add holdings Tesla, Inc. designs, develops, manufactures, leases, and sells electric vehicles, and energy generation and storage systems in the United States, China, and internationally. The company operates in two segments, Automotive; and Energy Generation and Storage. The Automotive segment offers electric vehicles, as well as sells automotive regulatory credits; and non-warranty after-sales vehicle, used vehicles, body shop and parts, supercharging, retail merchandise, and vehicle insurance services. This segment also provides sedans and sport utility vehicles through direct and used vehicle sales, a network of Tesla Superchargers, and in-app upgrades; purchase financing and leasing services; services for electric vehicles through its company-owned service locations and Tesla mobile service technicians; and vehicle limited warranties and extended service plans. The Energy Generation and Storage segment engages in the design, manufacture, installation, sale, and leasing of solar energy generation and energy storage products, and related services to residential, commercial, and industrial customers and utilities through its website, stores, and galleries, as well as through a network of channel partners. This segment also provides services and repairs to its energy product customers, including under warranty; and various financing options to its residential customers. The company was formerly known as Tesla Motors, Inc. and changed its name to Tesla, Inc. in February 2017. Tesla, Inc. was incorporated in 2003 and is headquartered in Austin, Texas. October 22, 2025 at 8:00 PM UTC Tesla, Inc. Earnings Date September 8, 2025 at 12:00 AM UTC DEFA14A: Proxy Statements September 5, 2025 at 12:00 AM UTC PRE 14A: Proxy Statements August 4, 2025 at 12:00 AM UTC 8-K: Corporate Changes & Voting Matters July 24, 2025 at 12:00 AM UTC 10-Q: Periodic Financial Reports July 23, 2025 at 12:00 AM UTC 8-K: Corporate Changes & Voting Matters July 10, 2025 at 12:00 AM UTC 8-K: Corporate Changes & Voting Matters July 2, 2025 at 12:00 AM UTC 8-K: Corporate Changes & Voting Matters May 30, 2025 at 12:00 AM UTC SD: Specialized Disclosure Report filed pursuant to Section 1502 of the Dodd-Frank Wall Street Reform and Consumer Protection Act relating to the use of conflict minerals (Rule 13p-1) May 16, 2025 at 12:00 AM UTC 8-K: Corporate Changes & Voting Matters April 30, 2025 at 12:00 AM UTC 10-K/A: Periodic Financial Reports Sign in to access your portfolio'}, {'url': 'https://www.forbes.com/companies/tesla/', 'title': 'Tesla | TSLA Stock Price, Company Overview & News', 'text': 'Upgrade to gain access to exclusive features and add your profile photo. Learn More Forbes does not accept payment for placement on lists.'}]\n",
            "\n",
            "--- Generated Use Cases ---\n",
            "{'title': 'Predictive Maintenance for Vehicles', 'description': 'Utilize AI to analyze vehicle performance data and predict maintenance needs before they become critical.', 'data sources': 'Vehicle telemetry data, service history, customer feedback, warranty claims data.', 'impact': 'High', 'complexity': 'Medium'}\n",
            "{'title': 'Personalized Customer Experience through AI Chatbots', 'description': 'Implement AI-driven chatbots to provide personalized assistance and support to customers during their car purchasing and after-sales processes.', 'data sources': 'Customer data, website logs, sales data, FAQs, customer service transcripts.', 'impact': 'Medium', 'complexity': 'Medium'}\n",
            "{'title': 'Dynamic Pricing for Energy Services', 'description': 'Use AI algorithms to analyze demand, weather patterns, and customer usage to dynamically price energy generation and storage products.', 'data sources': 'Energy usage data, weather data, market trends, customer demographics.', 'impact': 'High', 'complexity': 'High'}\n",
            "{'title': 'Enhanced Supercharger Network Optimization', 'description': 'Leverage AI to optimize the placement and operation of Supercharger stations based on real-time data regarding traffic patterns and user charging behavior.', 'data sources': 'Traffic data, Supercharger usage data, demographic data, geographic data.', 'impact': 'High', 'complexity': 'Medium'}\n",
            "{'title': 'AI-Driven Customer Segmentation for Targeted Marketing', 'description': 'Implement machine learning models to segment customers based on buying behavior and preferences to enhance marketing efforts for vehicles and energy products.', 'data sources': 'Customer purchase history, website engagement data, demographic information, customer surveys.', 'impact': 'Medium', 'complexity': 'Medium'}\n"
          ]
        }
      ],
      "source": [
        "# 1. Call the research_agent_run function with a sample company name\n",
        "sample_company_name = \"tesla\"\n",
        "sample_research_docs = research_agent_run(sample_company_name)\n",
        "\n",
        "# 2. Print the sample_research_docs variable to inspect its content\n",
        "print(\"\\n--- Research Documents ---\")\n",
        "print(sample_research_docs)\n",
        "\n",
        "# 3. Call the usecase_agent_generate_usecases function\n",
        "sample_generated_use_cases = usecase_agent_generate_usecases(sample_company_name, sample_research_docs)\n",
        "\n",
        "# 4. Print the sample_generated_use_cases variable to inspect its content\n",
        "print(\"\\n--- Generated Use Cases ---\")\n",
        "for uc in sample_generated_use_cases:\n",
        "    print(uc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RGm6vw6JrmS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
